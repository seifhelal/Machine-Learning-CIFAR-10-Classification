{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from fullyconnectednet import *\n",
    "from trainer import Trainer\n",
    "from data_utils import load_CIFAR10\n",
    "from scipy import flip\n",
    "import random\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the raw CIFAR-10 data.\n",
    "cifar10_dir = 'cifar-10-batches-py'\n",
    "X_train, Y_train, X_test, Y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "#flip the images as a sort of preprocessing\n",
    "X_train_flipped= flip(X_train, 2)\n",
    "\n",
    "\n",
    "X_train_total= np.concatenate((X_train, X_train_flipped), axis=0)\n",
    "Y_train_total= np.concatenate((Y_train, Y_train), axis=0)\n",
    "Y_train_total, X_train_total=shuffle(Y_train_total, X_train_total, random_state=0)\n",
    "\n",
    "data= {}\n",
    "data['X_train'] = X_train_total[2000:100000]\n",
    "data['y_train'] = Y_train_total[2000:100000]\n",
    "data['X_val'] = X_train_total[:2000]\n",
    "data['y_val'] = Y_train_total[:2000]\n",
    "data['X_test'] = X_test\n",
    "data['y_test'] = Y_test\n",
    "\n",
    "#get a small part of the data to be used to choose the best hyper parameters\n",
    "num_train = 4000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (100000, 32, 32, 3)\n",
      "Training labels shape:  (100000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# As a sanity check, we print out the size of the training and test data.\n",
    "print ('Training data shape: ', X_train_total.shape)\n",
    "print ('Training labels shape: ', Y_train_total.shape)\n",
    "print ('Test data shape: ', X_test.shape)\n",
    "print ('Test labels shape: ', Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for adam optimizer : \n",
      "(Iteration 1 / 400) loss: 9.161659\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Iteration 11 / 400) loss: 3.174016\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Iteration 21 / 400) loss: 2.572888\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Iteration 31 / 400) loss: 2.410529\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Epoch 0 / 10) train acc: 0.089000; val_acc: 0.081000\n",
      "(Iteration 41 / 400) loss: 2.255265\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Iteration 51 / 400) loss: 2.210031\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Iteration 61 / 400) loss: 2.094036\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Iteration 71 / 400) loss: 2.126508\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.159000\n",
      "(Iteration 81 / 400) loss: 2.002189\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Iteration 91 / 400) loss: 2.045053\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Iteration 101 / 400) loss: 2.017577\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Iteration 111 / 400) loss: 1.987221\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Epoch 2 / 10) train acc: 0.279000; val_acc: 0.244500\n",
      "(Iteration 121 / 400) loss: 2.114928\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Iteration 131 / 400) loss: 2.016293\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Iteration 141 / 400) loss: 1.903364\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Iteration 151 / 400) loss: 1.983449\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Epoch 3 / 10) train acc: 0.260000; val_acc: 0.260000\n",
      "(Iteration 161 / 400) loss: 2.058089\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Iteration 171 / 400) loss: 1.927124\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Iteration 181 / 400) loss: 1.891994\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Iteration 191 / 400) loss: 1.943059\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Epoch 4 / 10) train acc: 0.286000; val_acc: 0.269000\n",
      "(Iteration 201 / 400) loss: 1.778684\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Iteration 211 / 400) loss: 1.838576\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Iteration 221 / 400) loss: 1.793074\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Iteration 231 / 400) loss: 1.772650\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Epoch 5 / 10) train acc: 0.331000; val_acc: 0.298000\n",
      "(Iteration 241 / 400) loss: 2.009426\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Iteration 251 / 400) loss: 1.973189\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Iteration 261 / 400) loss: 1.781301\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Iteration 271 / 400) loss: 1.771374\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Epoch 6 / 10) train acc: 0.290000; val_acc: 0.256000\n",
      "(Iteration 281 / 400) loss: 1.791574\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Iteration 291 / 400) loss: 1.832578\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Iteration 301 / 400) loss: 1.790234\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Iteration 311 / 400) loss: 1.740731\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.338000; val_acc: 0.307000\n",
      "(Iteration 321 / 400) loss: 1.897549\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Iteration 331 / 400) loss: 1.866078\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Iteration 341 / 400) loss: 1.901841\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Iteration 351 / 400) loss: 1.920380\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Epoch 8 / 10) train acc: 0.322000; val_acc: 0.278000\n",
      "(Iteration 361 / 400) loss: 1.837002\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Iteration 371 / 400) loss: 1.738107\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Iteration 381 / 400) loss: 1.851104\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Iteration 391 / 400) loss: 2.108303\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "(Epoch 9 / 10) train acc: 0.311000; val_acc: 0.298500\n",
      "for rmsprop optimizer : \n",
      "(Iteration 1 / 400) loss: 8.537849\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Iteration 11 / 400) loss: 2.321347\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Iteration 21 / 400) loss: 2.243171\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Iteration 31 / 400) loss: 2.225697\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Epoch 0 / 10) train acc: 0.118000; val_acc: 0.097000\n",
      "(Iteration 41 / 400) loss: 2.189725\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 51 / 400) loss: 2.070014\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Iteration 61 / 400) loss: 2.166302\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Iteration 71 / 400) loss: 2.125013\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Epoch 1 / 10) train acc: 0.176000; val_acc: 0.175500\n",
      "(Iteration 81 / 400) loss: 2.051140\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Iteration 91 / 400) loss: 1.936104\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Iteration 101 / 400) loss: 2.023172\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Iteration 111 / 400) loss: 1.968893\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Epoch 2 / 10) train acc: 0.235000; val_acc: 0.214000\n",
      "(Iteration 121 / 400) loss: 1.898383\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Iteration 131 / 400) loss: 1.961277\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Iteration 141 / 400) loss: 1.989797\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Iteration 151 / 400) loss: 1.958061\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Epoch 3 / 10) train acc: 0.256000; val_acc: 0.234500\n",
      "(Iteration 161 / 400) loss: 2.127474\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Iteration 171 / 400) loss: 2.033063\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Iteration 181 / 400) loss: 1.964534\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Iteration 191 / 400) loss: 1.916549\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Epoch 4 / 10) train acc: 0.284000; val_acc: 0.239500\n",
      "(Iteration 201 / 400) loss: 2.012662\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Iteration 211 / 400) loss: 1.790008\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Iteration 221 / 400) loss: 2.004074\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Iteration 231 / 400) loss: 1.805438\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Epoch 5 / 10) train acc: 0.311000; val_acc: 0.255000\n",
      "(Iteration 241 / 400) loss: 1.789693\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Iteration 251 / 400) loss: 1.776330\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Iteration 261 / 400) loss: 1.962460\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Iteration 271 / 400) loss: 1.807529\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Epoch 6 / 10) train acc: 0.321000; val_acc: 0.268000\n",
      "(Iteration 281 / 400) loss: 1.937791\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Iteration 291 / 400) loss: 1.737813\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Iteration 301 / 400) loss: 1.766446\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Iteration 311 / 400) loss: 1.831225\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Epoch 7 / 10) train acc: 0.319000; val_acc: 0.275000\n",
      "(Iteration 321 / 400) loss: 1.835633\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Iteration 331 / 400) loss: 1.858631\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Iteration 341 / 400) loss: 1.974487\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 351 / 400) loss: 1.829496\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Epoch 8 / 10) train acc: 0.321000; val_acc: 0.269500\n",
      "(Iteration 361 / 400) loss: 1.648152\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Iteration 371 / 400) loss: 1.805429\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Iteration 381 / 400) loss: 1.856574\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Iteration 391 / 400) loss: 1.868268\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n",
      "(Epoch 9 / 10) train acc: 0.349000; val_acc: 0.287500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/air/anaconda3/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:106: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3QAAANsCAYAAAATFepNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1514aa7c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#finding the best GD optimizer model to be used\n",
    "training_dict={}\n",
    "layers=[85, 85, 80, 80]\n",
    "learning_rates = {'rmsprop': 1e-4, 'adam': 1e-3}\n",
    "for gs in ['adam', 'rmsprop']:\n",
    "    print(\"for %s optimizer : \"% gs)\n",
    "    model = FullyConnectedNet(layers, weight_scale=5e-2)\n",
    "\n",
    "    training = Trainer(model, small_data,\n",
    "                  num_epochs=10, batch_size=100,\n",
    "                  update_rule=gs,\n",
    "                  optim_config={\n",
    "                    'learning_rate': learning_rates[gs]\n",
    "                  },\n",
    "                  verbose=True)\n",
    "    training_dict[gs] = training\n",
    "    training.train()\n",
    "    \n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "for gs, training in training_dict.items():\n",
    "  plt.subplot(3, 1, 1)\n",
    "  plt.plot(training.loss_history, 'o', label=gs)\n",
    "  \n",
    "  plt.subplot(3, 1, 2)\n",
    "  plt.plot(training.train_acc_history, '-o', label=gs)\n",
    "\n",
    "  plt.subplot(3, 1, 3)\n",
    "  plt.plot(training.val_acc_history, '-o', label=gs)\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best valid acc yet: 0.199500   learning_rate: 1.000000e-04 weight_scale: 1.000000e-02\n",
      "best valid acc yet: 0.278500   learning_rate: 1.000000e-04 weight_scale: 1.359356e-02\n",
      "best valid acc yet: 0.300000   learning_rate: 1.000000e-04 weight_scale: 1.847850e-02\n",
      "best valid acc yet: 0.337500   learning_rate: 1.000000e-04 weight_scale: 2.511886e-02\n",
      "best valid acc yet: 0.345000   learning_rate: 1.533608e-04 weight_scale: 2.511886e-02\n",
      "best valid acc yet: 0.349000   learning_rate: 2.351953e-04 weight_scale: 2.511886e-02\n",
      "best valid acc yet: 0.349500   learning_rate: 3.606973e-04 weight_scale: 2.511886e-02\n"
     ]
    }
   ],
   "source": [
    "#find the best learning rate for \n",
    "learning_rates = np.logspace(-4, -2.7, 8)\n",
    "weight_scales = np.logspace(-2, -1.6, 4)\n",
    "best_model = None\n",
    "best_acc = -1\n",
    "for learning_rate in learning_rates:\n",
    "    for weight_scale in weight_scales:\n",
    "        model = FullyConnectedNet(layers, weight_scale=weight_scale)\n",
    "\n",
    "        training = Trainer(model, small_data,\n",
    "                  num_epochs=10, batch_size=100,\n",
    "                  update_rule='adam',\n",
    "                  optim_config={\n",
    "                    'learning_rate': learning_rate\n",
    "                  },\n",
    "                  verbose=False)\n",
    "        training.train()\n",
    "        if training.best_val_acc > best_acc:\n",
    "            best_acc = training.best_val_acc\n",
    "            best_model = model\n",
    "            best_learning_rate, best_weight_scale = (learning_rate, weight_scale)\n",
    "            print(\"best valid acc yet: %f   learning_rate: %e weight_scale: %e\" % (best_acc, learning_rate, weight_scale))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for regulraization factor 0.010000  : \n",
      "(Iteration 1 / 400) loss: 3.201798\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Iteration 11 / 400) loss: 3.005573\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Iteration 21 / 400) loss: 2.936881\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Iteration 31 / 400) loss: 2.699456\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Epoch 0 / 10) train acc: 0.097000; val_acc: 0.096500\n",
      "(Iteration 41 / 400) loss: 2.805976\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Iteration 51 / 400) loss: 2.551252\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Iteration 61 / 400) loss: 2.554654\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Iteration 71 / 400) loss: 2.415688\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Epoch 1 / 10) train acc: 0.245000; val_acc: 0.252500\n",
      "(Iteration 81 / 400) loss: 2.506844\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Iteration 91 / 400) loss: 2.344030\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Iteration 101 / 400) loss: 2.429637\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Iteration 111 / 400) loss: 2.274118\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Epoch 2 / 10) train acc: 0.281000; val_acc: 0.247500\n",
      "(Iteration 121 / 400) loss: 2.331658\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Iteration 131 / 400) loss: 2.287856\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Iteration 141 / 400) loss: 2.267378\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Iteration 151 / 400) loss: 2.284755\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Epoch 3 / 10) train acc: 0.339000; val_acc: 0.301000\n",
      "(Iteration 161 / 400) loss: 2.229077\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Iteration 171 / 400) loss: 2.329112\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Iteration 181 / 400) loss: 2.096953\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Iteration 191 / 400) loss: 2.041757\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Epoch 4 / 10) train acc: 0.304000; val_acc: 0.299000\n",
      "(Iteration 201 / 400) loss: 2.172074\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Iteration 211 / 400) loss: 2.217556\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Iteration 221 / 400) loss: 2.177915\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Iteration 231 / 400) loss: 2.115758\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Epoch 5 / 10) train acc: 0.301000; val_acc: 0.308000\n",
      "(Iteration 241 / 400) loss: 2.230027\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Iteration 251 / 400) loss: 2.181614\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Iteration 261 / 400) loss: 2.277862\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Iteration 271 / 400) loss: 2.009605\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Epoch 6 / 10) train acc: 0.355000; val_acc: 0.303000\n",
      "(Iteration 281 / 400) loss: 2.191059\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Iteration 291 / 400) loss: 2.017274\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Iteration 301 / 400) loss: 2.145012\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Iteration 311 / 400) loss: 1.948838\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Epoch 7 / 10) train acc: 0.364000; val_acc: 0.325500\n",
      "(Iteration 321 / 400) loss: 2.063855\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Iteration 331 / 400) loss: 2.015028\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Iteration 341 / 400) loss: 1.965166\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Iteration 351 / 400) loss: 1.913947\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Epoch 8 / 10) train acc: 0.363000; val_acc: 0.322500\n",
      "(Iteration 361 / 400) loss: 1.886447\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Iteration 371 / 400) loss: 1.939047\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Iteration 381 / 400) loss: 1.939912\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Iteration 391 / 400) loss: 1.933194\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "(Epoch 9 / 10) train acc: 0.351000; val_acc: 0.331000\n",
      "for regulraization factor 0.019307  : \n",
      "(Iteration 1 / 400) loss: 4.027223\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Iteration 11 / 400) loss: 3.786270\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Iteration 21 / 400) loss: 3.502148\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Iteration 31 / 400) loss: 3.355654\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Epoch 0 / 10) train acc: 0.155000; val_acc: 0.142000\n",
      "(Iteration 41 / 400) loss: 3.094466\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Iteration 51 / 400) loss: 3.114235\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Iteration 61 / 400) loss: 2.757879\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Iteration 71 / 400) loss: 2.754380\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Epoch 1 / 10) train acc: 0.207000; val_acc: 0.206500\n",
      "(Iteration 81 / 400) loss: 2.600125\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Iteration 91 / 400) loss: 2.756307\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Iteration 101 / 400) loss: 2.503395\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Iteration 111 / 400) loss: 2.548568\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Epoch 2 / 10) train acc: 0.301000; val_acc: 0.261000\n",
      "(Iteration 121 / 400) loss: 2.527920\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Iteration 131 / 400) loss: 2.305848\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Iteration 141 / 400) loss: 2.364486\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Iteration 151 / 400) loss: 2.340101\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.263000; val_acc: 0.236500\n",
      "(Iteration 161 / 400) loss: 2.285914\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Iteration 171 / 400) loss: 2.340858\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Iteration 181 / 400) loss: 2.368086\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Iteration 191 / 400) loss: 2.091582\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n",
      "(Epoch 4 / 10) train acc: 0.300000; val_acc: 0.287000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 201 / 400) loss: 2.225578\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Iteration 211 / 400) loss: 2.251183\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Iteration 221 / 400) loss: 2.222379\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Iteration 231 / 400) loss: 2.106724\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Epoch 5 / 10) train acc: 0.352000; val_acc: 0.307000\n",
      "(Iteration 241 / 400) loss: 2.196230\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Iteration 251 / 400) loss: 2.060328\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Iteration 261 / 400) loss: 2.216838\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Iteration 271 / 400) loss: 2.130080\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Epoch 6 / 10) train acc: 0.358000; val_acc: 0.312000\n",
      "(Iteration 281 / 400) loss: 2.112018\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Iteration 291 / 400) loss: 2.136408\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Iteration 301 / 400) loss: 2.049405\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Iteration 311 / 400) loss: 2.121680\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Epoch 7 / 10) train acc: 0.304000; val_acc: 0.298500\n",
      "(Iteration 321 / 400) loss: 2.140128\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Iteration 331 / 400) loss: 2.023311\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Iteration 341 / 400) loss: 1.885049\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Iteration 351 / 400) loss: 1.894540\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Epoch 8 / 10) train acc: 0.357000; val_acc: 0.306000\n",
      "(Iteration 361 / 400) loss: 2.213998\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Iteration 371 / 400) loss: 2.096439\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Iteration 381 / 400) loss: 1.926718\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Iteration 391 / 400) loss: 2.088460\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "(Epoch 9 / 10) train acc: 0.334000; val_acc: 0.314000\n",
      "for regulraization factor 0.037276  : \n",
      "(Iteration 1 / 400) loss: 5.661344\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Iteration 11 / 400) loss: 5.093402\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Iteration 21 / 400) loss: 4.531091\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Iteration 31 / 400) loss: 4.203527\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Epoch 0 / 10) train acc: 0.132000; val_acc: 0.103000\n",
      "(Iteration 41 / 400) loss: 3.766722\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Iteration 51 / 400) loss: 3.447395\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Iteration 61 / 400) loss: 3.269616\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Iteration 71 / 400) loss: 3.061922\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Epoch 1 / 10) train acc: 0.234000; val_acc: 0.222000\n",
      "(Iteration 81 / 400) loss: 3.097670\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Iteration 91 / 400) loss: 2.897218\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Iteration 101 / 400) loss: 2.852323\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Iteration 111 / 400) loss: 2.777429\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Epoch 2 / 10) train acc: 0.245000; val_acc: 0.229000\n",
      "(Iteration 121 / 400) loss: 2.741098\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Iteration 131 / 400) loss: 2.576620\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Iteration 141 / 400) loss: 2.567126\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Iteration 151 / 400) loss: 2.648242\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Epoch 3 / 10) train acc: 0.282000; val_acc: 0.268000\n",
      "(Iteration 161 / 400) loss: 2.396194\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Iteration 171 / 400) loss: 2.368763\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Iteration 181 / 400) loss: 2.268177\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Iteration 191 / 400) loss: 2.347064\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Epoch 4 / 10) train acc: 0.318000; val_acc: 0.287500\n",
      "(Iteration 201 / 400) loss: 2.357859\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Iteration 211 / 400) loss: 2.326348\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Iteration 221 / 400) loss: 2.423386\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Iteration 231 / 400) loss: 2.328433\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.348000; val_acc: 0.305000\n",
      "(Iteration 241 / 400) loss: 2.318725\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Iteration 251 / 400) loss: 2.331224\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Iteration 261 / 400) loss: 2.166131\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Iteration 271 / 400) loss: 2.202985\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Epoch 6 / 10) train acc: 0.296000; val_acc: 0.280500\n",
      "(Iteration 281 / 400) loss: 2.136298\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Iteration 291 / 400) loss: 2.196749\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Iteration 301 / 400) loss: 2.050587\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Iteration 311 / 400) loss: 2.165560\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Epoch 7 / 10) train acc: 0.366000; val_acc: 0.301000\n",
      "(Iteration 321 / 400) loss: 2.240257\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Iteration 331 / 400) loss: 2.225348\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Iteration 341 / 400) loss: 2.113225\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Iteration 351 / 400) loss: 2.354863\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Epoch 8 / 10) train acc: 0.300000; val_acc: 0.291000\n",
      "(Iteration 361 / 400) loss: 2.027466\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Iteration 371 / 400) loss: 2.000552\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Iteration 381 / 400) loss: 2.147367\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Iteration 391 / 400) loss: 2.003680\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "(Epoch 9 / 10) train acc: 0.346000; val_acc: 0.324500\n",
      "for regulraization factor 0.071969  : \n",
      "(Iteration 1 / 400) loss: 8.696332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Iteration 11 / 400) loss: 7.580056\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Iteration 21 / 400) loss: 6.540177\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Iteration 31 / 400) loss: 5.679479\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Epoch 0 / 10) train acc: 0.084000; val_acc: 0.090500\n",
      "(Iteration 41 / 400) loss: 5.028544\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Iteration 51 / 400) loss: 4.592535\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Iteration 61 / 400) loss: 4.127261\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Iteration 71 / 400) loss: 3.816728\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Epoch 1 / 10) train acc: 0.215000; val_acc: 0.191500\n",
      "(Iteration 81 / 400) loss: 3.546662\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Iteration 91 / 400) loss: 3.315133\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Iteration 101 / 400) loss: 3.279736\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Iteration 111 / 400) loss: 3.214228\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.234500\n",
      "(Iteration 121 / 400) loss: 2.884421\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Iteration 131 / 400) loss: 2.746779\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Iteration 141 / 400) loss: 2.860218\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Iteration 151 / 400) loss: 2.704447\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Epoch 3 / 10) train acc: 0.324000; val_acc: 0.289000\n",
      "(Iteration 161 / 400) loss: 2.678249\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Iteration 171 / 400) loss: 2.566870\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Iteration 181 / 400) loss: 2.637716\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Iteration 191 / 400) loss: 2.428300\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Epoch 4 / 10) train acc: 0.302000; val_acc: 0.276000\n",
      "(Iteration 201 / 400) loss: 2.272795\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Iteration 211 / 400) loss: 2.410738\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Iteration 221 / 400) loss: 2.271849\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Iteration 231 / 400) loss: 2.327569\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Epoch 5 / 10) train acc: 0.312000; val_acc: 0.266500\n",
      "(Iteration 241 / 400) loss: 2.238952\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Iteration 251 / 400) loss: 2.313463\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Iteration 261 / 400) loss: 2.195958\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Iteration 271 / 400) loss: 2.352338\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Epoch 6 / 10) train acc: 0.323000; val_acc: 0.309500\n",
      "(Iteration 281 / 400) loss: 2.068602\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Iteration 291 / 400) loss: 2.348116\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Iteration 301 / 400) loss: 2.298674\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Iteration 311 / 400) loss: 2.416345\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Epoch 7 / 10) train acc: 0.330000; val_acc: 0.307000\n",
      "(Iteration 321 / 400) loss: 2.160552\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Iteration 331 / 400) loss: 2.067681\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Iteration 341 / 400) loss: 2.156184\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Iteration 351 / 400) loss: 2.333996\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Epoch 8 / 10) train acc: 0.327000; val_acc: 0.316500\n",
      "(Iteration 361 / 400) loss: 2.232362\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Iteration 371 / 400) loss: 2.112136\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Iteration 381 / 400) loss: 2.105139\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Iteration 391 / 400) loss: 2.114218\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "(Epoch 9 / 10) train acc: 0.321000; val_acc: 0.319000\n",
      "for regulraization factor 0.138950  : \n",
      "(Iteration 1 / 400) loss: 14.642685\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Iteration 11 / 400) loss: 12.354133\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Iteration 21 / 400) loss: 10.294869\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Iteration 31 / 400) loss: 8.577149\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Epoch 0 / 10) train acc: 0.100000; val_acc: 0.104500\n",
      "(Iteration 41 / 400) loss: 7.361994\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Iteration 51 / 400) loss: 6.400982\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Iteration 61 / 400) loss: 5.561367\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Iteration 71 / 400) loss: 5.037407\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Epoch 1 / 10) train acc: 0.187000; val_acc: 0.168000\n",
      "(Iteration 81 / 400) loss: 4.522342\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Iteration 91 / 400) loss: 4.148815\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Iteration 101 / 400) loss: 3.847562\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Iteration 111 / 400) loss: 3.517685\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Epoch 2 / 10) train acc: 0.241000; val_acc: 0.219500\n",
      "(Iteration 121 / 400) loss: 3.436813\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Iteration 131 / 400) loss: 3.159287\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Iteration 141 / 400) loss: 2.972361\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Iteration 151 / 400) loss: 3.185257\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Epoch 3 / 10) train acc: 0.233000; val_acc: 0.236500\n",
      "(Iteration 161 / 400) loss: 2.827265\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Iteration 171 / 400) loss: 2.661021\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Iteration 181 / 400) loss: 2.542710\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Iteration 191 / 400) loss: 2.384438\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Epoch 4 / 10) train acc: 0.261000; val_acc: 0.279000\n",
      "(Iteration 201 / 400) loss: 2.520704\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Iteration 211 / 400) loss: 2.430805\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Iteration 221 / 400) loss: 2.412033\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Iteration 231 / 400) loss: 2.355712\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Epoch 5 / 10) train acc: 0.296000; val_acc: 0.280000\n",
      "(Iteration 241 / 400) loss: 2.417639\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Iteration 251 / 400) loss: 2.279327\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Iteration 261 / 400) loss: 2.372677\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Iteration 271 / 400) loss: 2.190911\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Epoch 6 / 10) train acc: 0.314000; val_acc: 0.276500\n",
      "(Iteration 281 / 400) loss: 2.324825\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Iteration 291 / 400) loss: 2.198692\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Iteration 301 / 400) loss: 2.135094\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Iteration 311 / 400) loss: 2.111101\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Epoch 7 / 10) train acc: 0.297000; val_acc: 0.276000\n",
      "(Iteration 321 / 400) loss: 2.255199\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Iteration 331 / 400) loss: 2.254258\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Iteration 341 / 400) loss: 2.271703\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Iteration 351 / 400) loss: 2.211890\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Epoch 8 / 10) train acc: 0.298000; val_acc: 0.279000\n",
      "(Iteration 361 / 400) loss: 2.107937\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Iteration 371 / 400) loss: 2.148314\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Iteration 381 / 400) loss: 2.170936\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Iteration 391 / 400) loss: 2.111286\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "(Epoch 9 / 10) train acc: 0.323000; val_acc: 0.295000\n",
      "for regulraization factor 0.268270  : \n",
      "(Iteration 1 / 400) loss: 26.326682\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Iteration 11 / 400) loss: 21.714593\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Iteration 21 / 400) loss: 17.627746\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Iteration 31 / 400) loss: 14.354882\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Epoch 0 / 10) train acc: 0.119000; val_acc: 0.100500\n",
      "(Iteration 41 / 400) loss: 11.766728\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Iteration 51 / 400) loss: 9.757744\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Iteration 61 / 400) loss: 8.239030\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Iteration 71 / 400) loss: 6.962577\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Epoch 1 / 10) train acc: 0.160000; val_acc: 0.154500\n",
      "(Iteration 81 / 400) loss: 6.077171\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Iteration 91 / 400) loss: 5.304099\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Iteration 101 / 400) loss: 4.772080\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Iteration 111 / 400) loss: 4.236393\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n",
      "(Epoch 2 / 10) train acc: 0.161000; val_acc: 0.160000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 121 / 400) loss: 3.839180\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Iteration 131 / 400) loss: 3.539426\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Iteration 141 / 400) loss: 3.290628\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Iteration 151 / 400) loss: 3.162843\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Epoch 3 / 10) train acc: 0.205000; val_acc: 0.190000\n",
      "(Iteration 161 / 400) loss: 2.900311\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Iteration 171 / 400) loss: 2.837564\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Iteration 181 / 400) loss: 2.823450\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Iteration 191 / 400) loss: 2.642421\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Epoch 4 / 10) train acc: 0.183000; val_acc: 0.188000\n",
      "(Iteration 201 / 400) loss: 2.557365\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Iteration 211 / 400) loss: 2.555703\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Iteration 221 / 400) loss: 2.427280\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Iteration 231 / 400) loss: 2.386132\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.190500\n",
      "(Iteration 241 / 400) loss: 2.305898\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Iteration 251 / 400) loss: 2.326427\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Iteration 261 / 400) loss: 2.405656\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Iteration 271 / 400) loss: 2.320431\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Epoch 6 / 10) train acc: 0.196000; val_acc: 0.178000\n",
      "(Iteration 281 / 400) loss: 2.256610\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Iteration 291 / 400) loss: 2.262651\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Iteration 301 / 400) loss: 2.264814\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Iteration 311 / 400) loss: 2.221943\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Epoch 7 / 10) train acc: 0.196000; val_acc: 0.184500\n",
      "(Iteration 321 / 400) loss: 2.312778\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Iteration 331 / 400) loss: 2.187486\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Iteration 341 / 400) loss: 2.305532\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Iteration 351 / 400) loss: 2.191233\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Epoch 8 / 10) train acc: 0.181000; val_acc: 0.160500\n",
      "(Iteration 361 / 400) loss: 2.212907\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Iteration 371 / 400) loss: 2.141885\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Iteration 381 / 400) loss: 2.203615\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Iteration 391 / 400) loss: 2.324292\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.182500\n",
      "for regulraization factor 0.517947  : \n",
      "(Iteration 1 / 400) loss: 48.517575\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Iteration 11 / 400) loss: 39.542197\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Iteration 21 / 400) loss: 31.650503\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Iteration 31 / 400) loss: 25.351405\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Epoch 0 / 10) train acc: 0.096000; val_acc: 0.099000\n",
      "(Iteration 41 / 400) loss: 20.429529\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Iteration 51 / 400) loss: 16.591999\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Iteration 61 / 400) loss: 13.583965\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Iteration 71 / 400) loss: 11.238437\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Epoch 1 / 10) train acc: 0.148000; val_acc: 0.139500\n",
      "(Iteration 81 / 400) loss: 9.399915\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Iteration 91 / 400) loss: 7.881328\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Iteration 101 / 400) loss: 6.729985\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Iteration 111 / 400) loss: 5.969622\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.139500\n",
      "(Iteration 121 / 400) loss: 5.301576\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Iteration 131 / 400) loss: 4.579824\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Iteration 141 / 400) loss: 4.142355\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Iteration 151 / 400) loss: 3.692708\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Epoch 3 / 10) train acc: 0.136000; val_acc: 0.143000\n",
      "(Iteration 161 / 400) loss: 3.480735\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Iteration 171 / 400) loss: 3.196186\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Iteration 181 / 400) loss: 3.048834\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Iteration 191 / 400) loss: 2.997375\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Epoch 4 / 10) train acc: 0.152000; val_acc: 0.174000\n",
      "(Iteration 201 / 400) loss: 2.850638\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Iteration 211 / 400) loss: 2.590693\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Iteration 221 / 400) loss: 2.607295\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Iteration 231 / 400) loss: 2.595291\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Epoch 5 / 10) train acc: 0.151000; val_acc: 0.170500\n",
      "(Iteration 241 / 400) loss: 2.411468\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Iteration 251 / 400) loss: 2.441004\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Iteration 261 / 400) loss: 2.367504\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Iteration 271 / 400) loss: 2.377808\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.188000\n",
      "(Iteration 281 / 400) loss: 2.326649\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Iteration 291 / 400) loss: 2.353041\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Iteration 301 / 400) loss: 2.316744\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Iteration 311 / 400) loss: 2.291314\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Epoch 7 / 10) train acc: 0.206000; val_acc: 0.185000\n",
      "(Iteration 321 / 400) loss: 2.304205\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Iteration 331 / 400) loss: 2.233913\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Iteration 341 / 400) loss: 2.203363\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Iteration 351 / 400) loss: 2.228944\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Epoch 8 / 10) train acc: 0.224000; val_acc: 0.179500\n",
      "(Iteration 361 / 400) loss: 2.292972\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Iteration 371 / 400) loss: 2.397656\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Iteration 381 / 400) loss: 2.291250\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Iteration 391 / 400) loss: 2.194762\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.179500\n",
      "for regulraization factor 1.000000  : \n",
      "(Iteration 1 / 400) loss: 91.284498\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Iteration 11 / 400) loss: 73.908385\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Iteration 21 / 400) loss: 58.654071\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Iteration 31 / 400) loss: 46.531899\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Epoch 0 / 10) train acc: 0.117000; val_acc: 0.116000\n",
      "(Iteration 41 / 400) loss: 37.042279\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Iteration 51 / 400) loss: 29.660899\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Iteration 61 / 400) loss: 23.898956\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Iteration 71 / 400) loss: 19.390192\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Epoch 1 / 10) train acc: 0.175000; val_acc: 0.173000\n",
      "(Iteration 81 / 400) loss: 15.852341\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Iteration 91 / 400) loss: 13.067250\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Iteration 101 / 400) loss: 10.867532\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Iteration 111 / 400) loss: 9.125278\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Epoch 2 / 10) train acc: 0.117000; val_acc: 0.115500\n",
      "(Iteration 121 / 400) loss: 7.744379\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Iteration 131 / 400) loss: 6.646624\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Iteration 141 / 400) loss: 5.773374\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Iteration 151 / 400) loss: 5.076770\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Epoch 3 / 10) train acc: 0.101000; val_acc: 0.105000\n",
      "(Iteration 161 / 400) loss: 4.523009\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Iteration 171 / 400) loss: 4.079786\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Iteration 181 / 400) loss: 3.726541\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Iteration 191 / 400) loss: 3.444342\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Epoch 4 / 10) train acc: 0.110000; val_acc: 0.105000\n",
      "(Iteration 201 / 400) loss: 3.217901\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Iteration 211 / 400) loss: 3.034511\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Iteration 221 / 400) loss: 2.889360\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Iteration 231 / 400) loss: 2.775387\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Epoch 5 / 10) train acc: 0.097000; val_acc: 0.105000\n",
      "(Iteration 241 / 400) loss: 2.681507\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Iteration 251 / 400) loss: 2.605044\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Iteration 261 / 400) loss: 2.547373\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Iteration 271 / 400) loss: 2.499702\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Epoch 6 / 10) train acc: 0.092000; val_acc: 0.105000\n",
      "(Iteration 281 / 400) loss: 2.460205\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Iteration 291 / 400) loss: 2.430971\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Iteration 301 / 400) loss: 2.403724\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Iteration 311 / 400) loss: 2.384383\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Epoch 7 / 10) train acc: 0.102000; val_acc: 0.103500\n",
      "(Iteration 321 / 400) loss: 2.367977\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Iteration 331 / 400) loss: 2.355368\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Iteration 341 / 400) loss: 2.344612\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Iteration 351 / 400) loss: 2.336057\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Epoch 8 / 10) train acc: 0.109000; val_acc: 0.103500\n",
      "(Iteration 361 / 400) loss: 2.328956\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Iteration 371 / 400) loss: 2.325151\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Iteration 381 / 400) loss: 2.321389\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Iteration 391 / 400) loss: 2.317459\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n",
      "(Epoch 9 / 10) train acc: 0.114000; val_acc: 0.103500\n"
     ]
    }
   ],
   "source": [
    "#getting the best regularization factor \n",
    "regular = np.logspace(-2, 0, 8)\n",
    "for reg in regular:\n",
    "        print ('for regulraization factor %f  : ' % reg)\n",
    "        model = FullyConnectedNet(layers,reg=reg, weight_scale=best_weight_scale)\n",
    "        training = Trainer(model, small_data,\n",
    "                  num_epochs=10, batch_size=100,\n",
    "                  update_rule='adam',\n",
    "                  optim_config={\n",
    "                    'learning_rate': best_learning_rate\n",
    "                  },\n",
    "                  verbose=True)\n",
    "        training.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We use batchnorm here\n",
      "(Iteration 1 / 5880) loss: 3.831031\n",
      "(Epoch 0 / 12) train acc: 0.150000; val_acc: 0.156000\n",
      "(Iteration 11 / 5880) loss: 3.415466\n",
      "(Iteration 21 / 5880) loss: 3.346379\n",
      "(Iteration 31 / 5880) loss: 3.325324\n",
      "(Iteration 41 / 5880) loss: 3.146744\n",
      "(Iteration 51 / 5880) loss: 3.112665\n",
      "(Iteration 61 / 5880) loss: 3.051353\n",
      "(Iteration 71 / 5880) loss: 3.055535\n",
      "(Iteration 81 / 5880) loss: 3.044042\n",
      "(Iteration 91 / 5880) loss: 3.108011\n",
      "(Iteration 101 / 5880) loss: 2.877255\n",
      "(Iteration 111 / 5880) loss: 2.945039\n",
      "(Iteration 121 / 5880) loss: 3.079717\n",
      "(Iteration 131 / 5880) loss: 2.986085\n",
      "(Iteration 141 / 5880) loss: 2.891723\n",
      "(Iteration 151 / 5880) loss: 2.891088\n",
      "(Iteration 161 / 5880) loss: 2.920778\n",
      "(Iteration 171 / 5880) loss: 2.819945\n",
      "(Iteration 181 / 5880) loss: 2.714578\n",
      "(Iteration 191 / 5880) loss: 2.820009\n",
      "(Iteration 201 / 5880) loss: 2.727090\n",
      "(Iteration 211 / 5880) loss: 2.729504\n",
      "(Iteration 221 / 5880) loss: 2.811356\n",
      "(Iteration 231 / 5880) loss: 2.722912\n",
      "(Iteration 241 / 5880) loss: 2.753926\n",
      "(Iteration 251 / 5880) loss: 2.588876\n",
      "(Iteration 261 / 5880) loss: 2.712697\n",
      "(Iteration 271 / 5880) loss: 2.669233\n",
      "(Iteration 281 / 5880) loss: 2.682137\n",
      "(Iteration 291 / 5880) loss: 2.681146\n",
      "(Iteration 301 / 5880) loss: 2.673943\n",
      "(Iteration 311 / 5880) loss: 2.562430\n",
      "(Iteration 321 / 5880) loss: 2.597422\n",
      "(Iteration 331 / 5880) loss: 2.514685\n",
      "(Iteration 341 / 5880) loss: 2.583316\n",
      "(Iteration 351 / 5880) loss: 2.524672\n",
      "(Iteration 361 / 5880) loss: 2.717765\n",
      "(Iteration 371 / 5880) loss: 2.552635\n",
      "(Iteration 381 / 5880) loss: 2.510987\n",
      "(Iteration 391 / 5880) loss: 2.419385\n",
      "(Iteration 401 / 5880) loss: 2.383957\n",
      "(Iteration 411 / 5880) loss: 2.491825\n",
      "(Iteration 421 / 5880) loss: 2.417337\n",
      "(Iteration 431 / 5880) loss: 2.539134\n",
      "(Iteration 441 / 5880) loss: 2.408295\n",
      "(Iteration 451 / 5880) loss: 2.384310\n",
      "(Iteration 461 / 5880) loss: 2.662434\n",
      "(Iteration 471 / 5880) loss: 2.422075\n",
      "(Iteration 481 / 5880) loss: 2.356749\n",
      "(Epoch 1 / 12) train acc: 0.537000; val_acc: 0.515000\n",
      "(Iteration 491 / 5880) loss: 2.306765\n",
      "(Iteration 501 / 5880) loss: 2.414947\n",
      "(Iteration 511 / 5880) loss: 2.305023\n",
      "(Iteration 521 / 5880) loss: 2.310274\n",
      "(Iteration 531 / 5880) loss: 2.347227\n",
      "(Iteration 541 / 5880) loss: 2.309016\n",
      "(Iteration 551 / 5880) loss: 2.346087\n",
      "(Iteration 561 / 5880) loss: 2.273947\n",
      "(Iteration 571 / 5880) loss: 2.173395\n",
      "(Iteration 581 / 5880) loss: 2.329280\n",
      "(Iteration 591 / 5880) loss: 2.323657\n",
      "(Iteration 601 / 5880) loss: 2.267597\n",
      "(Iteration 611 / 5880) loss: 2.206826\n",
      "(Iteration 621 / 5880) loss: 2.338172\n",
      "(Iteration 631 / 5880) loss: 2.222574\n",
      "(Iteration 641 / 5880) loss: 2.187632\n",
      "(Iteration 651 / 5880) loss: 2.137678\n",
      "(Iteration 661 / 5880) loss: 2.279466\n",
      "(Iteration 671 / 5880) loss: 2.253725\n",
      "(Iteration 681 / 5880) loss: 2.096118\n",
      "(Iteration 691 / 5880) loss: 2.201658\n",
      "(Iteration 701 / 5880) loss: 2.257300\n",
      "(Iteration 711 / 5880) loss: 2.181274\n",
      "(Iteration 721 / 5880) loss: 2.163126\n",
      "(Iteration 731 / 5880) loss: 2.159681\n",
      "(Iteration 741 / 5880) loss: 2.295847\n",
      "(Iteration 751 / 5880) loss: 2.092512\n",
      "(Iteration 761 / 5880) loss: 2.093681\n",
      "(Iteration 771 / 5880) loss: 2.132251\n",
      "(Iteration 781 / 5880) loss: 1.981291\n",
      "(Iteration 791 / 5880) loss: 2.038168\n",
      "(Iteration 801 / 5880) loss: 2.048457\n",
      "(Iteration 811 / 5880) loss: 2.114679\n",
      "(Iteration 821 / 5880) loss: 2.104131\n",
      "(Iteration 831 / 5880) loss: 2.139923\n",
      "(Iteration 841 / 5880) loss: 2.081799\n",
      "(Iteration 851 / 5880) loss: 1.949256\n",
      "(Iteration 861 / 5880) loss: 2.225660\n",
      "(Iteration 871 / 5880) loss: 2.144721\n",
      "(Iteration 881 / 5880) loss: 2.001789\n",
      "(Iteration 891 / 5880) loss: 2.142086\n",
      "(Iteration 901 / 5880) loss: 2.048761\n",
      "(Iteration 911 / 5880) loss: 2.123261\n",
      "(Iteration 921 / 5880) loss: 2.104312\n",
      "(Iteration 931 / 5880) loss: 1.938656\n",
      "(Iteration 941 / 5880) loss: 2.020825\n",
      "(Iteration 951 / 5880) loss: 1.943580\n",
      "(Iteration 961 / 5880) loss: 2.046609\n",
      "(Iteration 971 / 5880) loss: 2.044643\n",
      "(Epoch 2 / 12) train acc: 0.574000; val_acc: 0.550000\n",
      "(Iteration 981 / 5880) loss: 2.066691\n",
      "(Iteration 991 / 5880) loss: 2.170286\n",
      "(Iteration 1001 / 5880) loss: 1.961462\n",
      "(Iteration 1011 / 5880) loss: 1.975851\n",
      "(Iteration 1021 / 5880) loss: 2.024548\n",
      "(Iteration 1031 / 5880) loss: 2.114477\n",
      "(Iteration 1041 / 5880) loss: 2.117565\n",
      "(Iteration 1051 / 5880) loss: 1.989060\n",
      "(Iteration 1061 / 5880) loss: 1.978549\n",
      "(Iteration 1071 / 5880) loss: 1.922130\n",
      "(Iteration 1081 / 5880) loss: 1.876575\n",
      "(Iteration 1091 / 5880) loss: 1.937355\n",
      "(Iteration 1101 / 5880) loss: 1.964799\n",
      "(Iteration 1111 / 5880) loss: 1.926963\n",
      "(Iteration 1121 / 5880) loss: 1.993869\n",
      "(Iteration 1131 / 5880) loss: 1.945597\n",
      "(Iteration 1141 / 5880) loss: 1.893273\n",
      "(Iteration 1151 / 5880) loss: 1.954173\n",
      "(Iteration 1161 / 5880) loss: 1.905152\n",
      "(Iteration 1171 / 5880) loss: 1.865321\n",
      "(Iteration 1181 / 5880) loss: 1.928407\n",
      "(Iteration 1191 / 5880) loss: 1.950128\n",
      "(Iteration 1201 / 5880) loss: 2.003612\n",
      "(Iteration 1211 / 5880) loss: 1.922420\n",
      "(Iteration 1221 / 5880) loss: 1.924673\n",
      "(Iteration 1231 / 5880) loss: 1.884450\n",
      "(Iteration 1241 / 5880) loss: 2.047339\n",
      "(Iteration 1251 / 5880) loss: 1.772682\n",
      "(Iteration 1261 / 5880) loss: 1.875213\n",
      "(Iteration 1271 / 5880) loss: 1.859693\n",
      "(Iteration 1281 / 5880) loss: 1.851957\n",
      "(Iteration 1291 / 5880) loss: 1.918622\n",
      "(Iteration 1301 / 5880) loss: 1.888857\n",
      "(Iteration 1311 / 5880) loss: 1.866488\n",
      "(Iteration 1321 / 5880) loss: 1.954785\n",
      "(Iteration 1331 / 5880) loss: 1.850825\n",
      "(Iteration 1341 / 5880) loss: 1.880624\n",
      "(Iteration 1351 / 5880) loss: 1.898083\n",
      "(Iteration 1361 / 5880) loss: 1.760537\n",
      "(Iteration 1371 / 5880) loss: 1.954551\n",
      "(Iteration 1381 / 5880) loss: 1.868884\n",
      "(Iteration 1391 / 5880) loss: 1.854253\n",
      "(Iteration 1401 / 5880) loss: 1.721080\n",
      "(Iteration 1411 / 5880) loss: 1.794554\n",
      "(Iteration 1421 / 5880) loss: 1.944921\n",
      "(Iteration 1431 / 5880) loss: 1.786768\n",
      "(Iteration 1441 / 5880) loss: 1.991708\n",
      "(Iteration 1451 / 5880) loss: 1.740391\n",
      "(Iteration 1461 / 5880) loss: 1.681040\n",
      "(Epoch 3 / 12) train acc: 0.617000; val_acc: 0.557500\n",
      "(Iteration 1471 / 5880) loss: 1.755043\n",
      "(Iteration 1481 / 5880) loss: 1.719037\n",
      "(Iteration 1491 / 5880) loss: 1.845692\n",
      "(Iteration 1501 / 5880) loss: 1.840138\n",
      "(Iteration 1511 / 5880) loss: 1.814936\n",
      "(Iteration 1521 / 5880) loss: 1.751036\n",
      "(Iteration 1531 / 5880) loss: 1.785629\n",
      "(Iteration 1541 / 5880) loss: 1.716831\n",
      "(Iteration 1551 / 5880) loss: 1.830160\n",
      "(Iteration 1561 / 5880) loss: 1.757633\n",
      "(Iteration 1571 / 5880) loss: 1.759494\n",
      "(Iteration 1581 / 5880) loss: 1.855558\n",
      "(Iteration 1591 / 5880) loss: 1.700149\n",
      "(Iteration 1601 / 5880) loss: 1.690477\n",
      "(Iteration 1611 / 5880) loss: 1.627369\n",
      "(Iteration 1621 / 5880) loss: 1.865686\n",
      "(Iteration 1631 / 5880) loss: 1.751374\n",
      "(Iteration 1641 / 5880) loss: 1.821319\n",
      "(Iteration 1651 / 5880) loss: 1.644192\n",
      "(Iteration 1661 / 5880) loss: 1.687679\n",
      "(Iteration 1671 / 5880) loss: 1.849727\n",
      "(Iteration 1681 / 5880) loss: 1.625840\n",
      "(Iteration 1691 / 5880) loss: 1.674563\n",
      "(Iteration 1701 / 5880) loss: 1.689511\n",
      "(Iteration 1711 / 5880) loss: 1.669249\n",
      "(Iteration 1721 / 5880) loss: 1.660963\n",
      "(Iteration 1731 / 5880) loss: 1.713628\n",
      "(Iteration 1741 / 5880) loss: 1.597498\n",
      "(Iteration 1751 / 5880) loss: 1.670817\n",
      "(Iteration 1761 / 5880) loss: 1.560072\n",
      "(Iteration 1771 / 5880) loss: 1.613393\n",
      "(Iteration 1781 / 5880) loss: 1.569528\n",
      "(Iteration 1791 / 5880) loss: 1.657563\n",
      "(Iteration 1801 / 5880) loss: 1.798497\n",
      "(Iteration 1811 / 5880) loss: 1.608646\n",
      "(Iteration 1821 / 5880) loss: 1.583319\n",
      "(Iteration 1831 / 5880) loss: 1.684416\n",
      "(Iteration 1841 / 5880) loss: 1.610063\n",
      "(Iteration 1851 / 5880) loss: 1.728484\n",
      "(Iteration 1861 / 5880) loss: 1.655619\n",
      "(Iteration 1871 / 5880) loss: 1.647334\n",
      "(Iteration 1881 / 5880) loss: 1.667143\n",
      "(Iteration 1891 / 5880) loss: 1.537257\n",
      "(Iteration 1901 / 5880) loss: 1.580273\n",
      "(Iteration 1911 / 5880) loss: 1.526409\n",
      "(Iteration 1921 / 5880) loss: 1.674970\n",
      "(Iteration 1931 / 5880) loss: 1.580449\n",
      "(Iteration 1941 / 5880) loss: 1.623958\n",
      "(Iteration 1951 / 5880) loss: 1.734693\n",
      "(Epoch 4 / 12) train acc: 0.640000; val_acc: 0.562500\n",
      "(Iteration 1961 / 5880) loss: 1.788768\n",
      "(Iteration 1971 / 5880) loss: 1.603217\n",
      "(Iteration 1981 / 5880) loss: 1.673823\n",
      "(Iteration 1991 / 5880) loss: 1.595268\n",
      "(Iteration 2001 / 5880) loss: 1.553079\n",
      "(Iteration 2011 / 5880) loss: 1.515524\n",
      "(Iteration 2021 / 5880) loss: 1.640117\n",
      "(Iteration 2031 / 5880) loss: 1.463422\n",
      "(Iteration 2041 / 5880) loss: 1.663669\n",
      "(Iteration 2051 / 5880) loss: 1.610166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 2061 / 5880) loss: 1.663326\n",
      "(Iteration 2071 / 5880) loss: 1.700847\n",
      "(Iteration 2081 / 5880) loss: 1.746529\n",
      "(Iteration 2091 / 5880) loss: 1.573655\n",
      "(Iteration 2101 / 5880) loss: 1.530241\n",
      "(Iteration 2111 / 5880) loss: 1.661644\n",
      "(Iteration 2121 / 5880) loss: 1.511103\n",
      "(Iteration 2131 / 5880) loss: 1.572256\n",
      "(Iteration 2141 / 5880) loss: 1.504131\n",
      "(Iteration 2151 / 5880) loss: 1.558327\n",
      "(Iteration 2161 / 5880) loss: 1.616684\n",
      "(Iteration 2171 / 5880) loss: 1.510038\n",
      "(Iteration 2181 / 5880) loss: 1.562454\n",
      "(Iteration 2191 / 5880) loss: 1.416121\n",
      "(Iteration 2201 / 5880) loss: 1.666794\n",
      "(Iteration 2211 / 5880) loss: 1.560774\n",
      "(Iteration 2221 / 5880) loss: 1.509103\n",
      "(Iteration 2231 / 5880) loss: 1.491113\n",
      "(Iteration 2241 / 5880) loss: 1.531528\n",
      "(Iteration 2251 / 5880) loss: 1.439056\n",
      "(Iteration 2261 / 5880) loss: 1.445584\n",
      "(Iteration 2271 / 5880) loss: 1.684196\n",
      "(Iteration 2281 / 5880) loss: 1.589605\n",
      "(Iteration 2291 / 5880) loss: 1.560937\n",
      "(Iteration 2301 / 5880) loss: 1.524798\n",
      "(Iteration 2311 / 5880) loss: 1.541989\n",
      "(Iteration 2321 / 5880) loss: 1.549048\n",
      "(Iteration 2331 / 5880) loss: 1.585431\n",
      "(Iteration 2341 / 5880) loss: 1.508970\n",
      "(Iteration 2351 / 5880) loss: 1.593824\n",
      "(Iteration 2361 / 5880) loss: 1.461355\n",
      "(Iteration 2371 / 5880) loss: 1.528425\n",
      "(Iteration 2381 / 5880) loss: 1.501744\n",
      "(Iteration 2391 / 5880) loss: 1.565625\n",
      "(Iteration 2401 / 5880) loss: 1.513333\n",
      "(Iteration 2411 / 5880) loss: 1.490490\n",
      "(Iteration 2421 / 5880) loss: 1.500896\n",
      "(Iteration 2431 / 5880) loss: 1.449646\n",
      "(Iteration 2441 / 5880) loss: 1.509563\n",
      "(Epoch 5 / 12) train acc: 0.696000; val_acc: 0.573000\n",
      "(Iteration 2451 / 5880) loss: 1.614746\n",
      "(Iteration 2461 / 5880) loss: 1.482239\n",
      "(Iteration 2471 / 5880) loss: 1.536233\n",
      "(Iteration 2481 / 5880) loss: 1.485347\n",
      "(Iteration 2491 / 5880) loss: 1.549358\n",
      "(Iteration 2501 / 5880) loss: 1.444702\n",
      "(Iteration 2511 / 5880) loss: 1.435270\n",
      "(Iteration 2521 / 5880) loss: 1.735835\n",
      "(Iteration 2531 / 5880) loss: 1.540242\n",
      "(Iteration 2541 / 5880) loss: 1.461924\n",
      "(Iteration 2551 / 5880) loss: 1.434994\n",
      "(Iteration 2561 / 5880) loss: 1.441506\n",
      "(Iteration 2571 / 5880) loss: 1.558087\n",
      "(Iteration 2581 / 5880) loss: 1.487120\n",
      "(Iteration 2591 / 5880) loss: 1.553686\n",
      "(Iteration 2601 / 5880) loss: 1.328139\n",
      "(Iteration 2611 / 5880) loss: 1.446582\n",
      "(Iteration 2621 / 5880) loss: 1.383765\n",
      "(Iteration 2631 / 5880) loss: 1.462798\n",
      "(Iteration 2641 / 5880) loss: 1.475859\n",
      "(Iteration 2651 / 5880) loss: 1.573424\n",
      "(Iteration 2661 / 5880) loss: 1.459767\n",
      "(Iteration 2671 / 5880) loss: 1.542232\n",
      "(Iteration 2681 / 5880) loss: 1.386651\n",
      "(Iteration 2691 / 5880) loss: 1.435526\n",
      "(Iteration 2701 / 5880) loss: 1.322563\n",
      "(Iteration 2711 / 5880) loss: 1.491259\n",
      "(Iteration 2721 / 5880) loss: 1.407828\n",
      "(Iteration 2731 / 5880) loss: 1.416881\n",
      "(Iteration 2741 / 5880) loss: 1.390039\n",
      "(Iteration 2751 / 5880) loss: 1.256198\n",
      "(Iteration 2761 / 5880) loss: 1.277090\n",
      "(Iteration 2771 / 5880) loss: 1.208560\n",
      "(Iteration 2781 / 5880) loss: 1.392959\n",
      "(Iteration 2791 / 5880) loss: 1.468686\n",
      "(Iteration 2801 / 5880) loss: 1.571409\n",
      "(Iteration 2811 / 5880) loss: 1.354166\n",
      "(Iteration 2821 / 5880) loss: 1.326677\n",
      "(Iteration 2831 / 5880) loss: 1.374578\n",
      "(Iteration 2841 / 5880) loss: 1.276493\n",
      "(Iteration 2851 / 5880) loss: 1.368530\n",
      "(Iteration 2861 / 5880) loss: 1.268764\n",
      "(Iteration 2871 / 5880) loss: 1.378391\n",
      "(Iteration 2881 / 5880) loss: 1.417009\n",
      "(Iteration 2891 / 5880) loss: 1.339868\n",
      "(Iteration 2901 / 5880) loss: 1.378264\n",
      "(Iteration 2911 / 5880) loss: 1.306377\n",
      "(Iteration 2921 / 5880) loss: 1.312541\n",
      "(Iteration 2931 / 5880) loss: 1.414983\n",
      "(Epoch 6 / 12) train acc: 0.695000; val_acc: 0.564000\n",
      "(Iteration 2941 / 5880) loss: 1.361155\n",
      "(Iteration 2951 / 5880) loss: 1.419602\n",
      "(Iteration 2961 / 5880) loss: 1.409446\n",
      "(Iteration 2971 / 5880) loss: 1.239758\n",
      "(Iteration 2981 / 5880) loss: 1.282743\n",
      "(Iteration 2991 / 5880) loss: 1.360826\n",
      "(Iteration 3001 / 5880) loss: 1.336832\n",
      "(Iteration 3011 / 5880) loss: 1.342233\n",
      "(Iteration 3021 / 5880) loss: 1.302950\n",
      "(Iteration 3031 / 5880) loss: 1.338400\n",
      "(Iteration 3041 / 5880) loss: 1.139405\n",
      "(Iteration 3051 / 5880) loss: 1.177872\n",
      "(Iteration 3061 / 5880) loss: 1.277395\n",
      "(Iteration 3071 / 5880) loss: 1.381530\n",
      "(Iteration 3081 / 5880) loss: 1.403515\n",
      "(Iteration 3091 / 5880) loss: 1.332190\n",
      "(Iteration 3101 / 5880) loss: 1.254080\n",
      "(Iteration 3111 / 5880) loss: 1.331167\n",
      "(Iteration 3121 / 5880) loss: 1.324271\n",
      "(Iteration 3131 / 5880) loss: 1.370618\n",
      "(Iteration 3141 / 5880) loss: 1.381265\n",
      "(Iteration 3151 / 5880) loss: 1.315390\n",
      "(Iteration 3161 / 5880) loss: 1.331066\n",
      "(Iteration 3171 / 5880) loss: 1.328368\n",
      "(Iteration 3181 / 5880) loss: 1.336972\n",
      "(Iteration 3191 / 5880) loss: 1.306586\n",
      "(Iteration 3201 / 5880) loss: 1.321030\n",
      "(Iteration 3211 / 5880) loss: 1.392881\n",
      "(Iteration 3221 / 5880) loss: 1.422183\n",
      "(Iteration 3231 / 5880) loss: 1.341751\n",
      "(Iteration 3241 / 5880) loss: 1.272149\n",
      "(Iteration 3251 / 5880) loss: 1.381511\n",
      "(Iteration 3261 / 5880) loss: 1.267117\n",
      "(Iteration 3271 / 5880) loss: 1.213369\n",
      "(Iteration 3281 / 5880) loss: 1.294423\n",
      "(Iteration 3291 / 5880) loss: 1.342602\n",
      "(Iteration 3301 / 5880) loss: 1.296364\n",
      "(Iteration 3311 / 5880) loss: 1.266114\n",
      "(Iteration 3321 / 5880) loss: 1.275294\n",
      "(Iteration 3331 / 5880) loss: 1.214288\n",
      "(Iteration 3341 / 5880) loss: 1.188883\n",
      "(Iteration 3351 / 5880) loss: 1.270234\n",
      "(Iteration 3361 / 5880) loss: 1.289207\n",
      "(Iteration 3371 / 5880) loss: 1.332190\n",
      "(Iteration 3381 / 5880) loss: 1.401420\n",
      "(Iteration 3391 / 5880) loss: 1.230785\n",
      "(Iteration 3401 / 5880) loss: 1.328846\n",
      "(Iteration 3411 / 5880) loss: 1.165823\n",
      "(Iteration 3421 / 5880) loss: 1.321670\n",
      "(Epoch 7 / 12) train acc: 0.762000; val_acc: 0.585000\n",
      "(Iteration 3431 / 5880) loss: 1.278443\n",
      "(Iteration 3441 / 5880) loss: 1.237979\n",
      "(Iteration 3451 / 5880) loss: 1.173103\n",
      "(Iteration 3461 / 5880) loss: 1.245645\n",
      "(Iteration 3471 / 5880) loss: 1.286240\n",
      "(Iteration 3481 / 5880) loss: 1.215110\n",
      "(Iteration 3491 / 5880) loss: 1.113749\n",
      "(Iteration 3501 / 5880) loss: 1.224674\n",
      "(Iteration 3511 / 5880) loss: 1.155516\n",
      "(Iteration 3521 / 5880) loss: 1.232810\n",
      "(Iteration 3531 / 5880) loss: 1.241784\n",
      "(Iteration 3541 / 5880) loss: 1.215697\n",
      "(Iteration 3551 / 5880) loss: 1.259187\n",
      "(Iteration 3561 / 5880) loss: 1.262689\n",
      "(Iteration 3571 / 5880) loss: 1.209131\n",
      "(Iteration 3581 / 5880) loss: 1.197750\n",
      "(Iteration 3591 / 5880) loss: 1.296379\n",
      "(Iteration 3601 / 5880) loss: 1.242056\n",
      "(Iteration 3611 / 5880) loss: 1.326297\n",
      "(Iteration 3621 / 5880) loss: 1.196316\n",
      "(Iteration 3631 / 5880) loss: 1.117068\n",
      "(Iteration 3641 / 5880) loss: 1.055902\n",
      "(Iteration 3651 / 5880) loss: 1.143313\n",
      "(Iteration 3661 / 5880) loss: 1.219656\n",
      "(Iteration 3671 / 5880) loss: 1.194284\n",
      "(Iteration 3681 / 5880) loss: 1.308868\n",
      "(Iteration 3691 / 5880) loss: 1.053342\n",
      "(Iteration 3701 / 5880) loss: 1.187799\n",
      "(Iteration 3711 / 5880) loss: 1.154798\n",
      "(Iteration 3721 / 5880) loss: 1.219625\n",
      "(Iteration 3731 / 5880) loss: 1.130240\n",
      "(Iteration 3741 / 5880) loss: 1.204461\n",
      "(Iteration 3751 / 5880) loss: 1.215872\n",
      "(Iteration 3761 / 5880) loss: 1.149097\n",
      "(Iteration 3771 / 5880) loss: 1.227618\n",
      "(Iteration 3781 / 5880) loss: 1.157770\n",
      "(Iteration 3791 / 5880) loss: 1.200602\n",
      "(Iteration 3801 / 5880) loss: 1.106098\n",
      "(Iteration 3811 / 5880) loss: 1.207580\n",
      "(Iteration 3821 / 5880) loss: 1.094422\n",
      "(Iteration 3831 / 5880) loss: 1.196852\n",
      "(Iteration 3841 / 5880) loss: 1.139726\n",
      "(Iteration 3851 / 5880) loss: 1.154650\n",
      "(Iteration 3861 / 5880) loss: 1.206634\n",
      "(Iteration 3871 / 5880) loss: 1.141099\n",
      "(Iteration 3881 / 5880) loss: 1.153235\n",
      "(Iteration 3891 / 5880) loss: 1.185045\n",
      "(Iteration 3901 / 5880) loss: 1.176323\n",
      "(Iteration 3911 / 5880) loss: 1.121041\n",
      "(Epoch 8 / 12) train acc: 0.746000; val_acc: 0.582000\n",
      "(Iteration 3921 / 5880) loss: 1.127713\n",
      "(Iteration 3931 / 5880) loss: 1.074737\n",
      "(Iteration 3941 / 5880) loss: 1.232968\n",
      "(Iteration 3951 / 5880) loss: 1.259593\n",
      "(Iteration 3961 / 5880) loss: 1.125277\n",
      "(Iteration 3971 / 5880) loss: 1.171774\n",
      "(Iteration 3981 / 5880) loss: 1.250282\n",
      "(Iteration 3991 / 5880) loss: 1.154607\n",
      "(Iteration 4001 / 5880) loss: 1.044291\n",
      "(Iteration 4011 / 5880) loss: 1.092585\n",
      "(Iteration 4021 / 5880) loss: 1.138534\n",
      "(Iteration 4031 / 5880) loss: 1.136951\n",
      "(Iteration 4041 / 5880) loss: 1.134524\n",
      "(Iteration 4051 / 5880) loss: 1.045214\n",
      "(Iteration 4061 / 5880) loss: 1.244976\n",
      "(Iteration 4071 / 5880) loss: 1.119396\n",
      "(Iteration 4081 / 5880) loss: 1.179385\n",
      "(Iteration 4091 / 5880) loss: 1.160922\n",
      "(Iteration 4101 / 5880) loss: 1.310322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 4111 / 5880) loss: 1.174097\n",
      "(Iteration 4121 / 5880) loss: 1.149349\n",
      "(Iteration 4131 / 5880) loss: 1.104766\n",
      "(Iteration 4141 / 5880) loss: 1.084305\n",
      "(Iteration 4151 / 5880) loss: 1.133298\n",
      "(Iteration 4161 / 5880) loss: 1.169880\n",
      "(Iteration 4171 / 5880) loss: 1.028493\n",
      "(Iteration 4181 / 5880) loss: 1.053925\n",
      "(Iteration 4191 / 5880) loss: 1.128039\n",
      "(Iteration 4201 / 5880) loss: 1.214866\n",
      "(Iteration 4211 / 5880) loss: 1.138651\n",
      "(Iteration 4221 / 5880) loss: 1.081275\n",
      "(Iteration 4231 / 5880) loss: 1.205212\n",
      "(Iteration 4241 / 5880) loss: 1.049158\n",
      "(Iteration 4251 / 5880) loss: 1.111999\n",
      "(Iteration 4261 / 5880) loss: 1.107794\n",
      "(Iteration 4271 / 5880) loss: 1.164402\n",
      "(Iteration 4281 / 5880) loss: 1.089844\n",
      "(Iteration 4291 / 5880) loss: 1.063869\n",
      "(Iteration 4301 / 5880) loss: 1.103590\n",
      "(Iteration 4311 / 5880) loss: 1.088598\n",
      "(Iteration 4321 / 5880) loss: 1.126480\n",
      "(Iteration 4331 / 5880) loss: 1.100691\n",
      "(Iteration 4341 / 5880) loss: 1.146740\n",
      "(Iteration 4351 / 5880) loss: 1.226079\n",
      "(Iteration 4361 / 5880) loss: 1.276798\n",
      "(Iteration 4371 / 5880) loss: 1.228291\n",
      "(Iteration 4381 / 5880) loss: 1.101069\n",
      "(Iteration 4391 / 5880) loss: 1.174162\n",
      "(Iteration 4401 / 5880) loss: 1.119880\n",
      "(Epoch 9 / 12) train acc: 0.807000; val_acc: 0.596500\n",
      "(Iteration 4411 / 5880) loss: 1.157238\n",
      "(Iteration 4421 / 5880) loss: 1.157954\n",
      "(Iteration 4431 / 5880) loss: 1.037876\n",
      "(Iteration 4441 / 5880) loss: 1.062510\n",
      "(Iteration 4451 / 5880) loss: 1.092178\n",
      "(Iteration 4461 / 5880) loss: 1.073775\n",
      "(Iteration 4471 / 5880) loss: 1.041956\n",
      "(Iteration 4481 / 5880) loss: 1.068005\n",
      "(Iteration 4491 / 5880) loss: 1.117885\n",
      "(Iteration 4501 / 5880) loss: 1.103636\n",
      "(Iteration 4511 / 5880) loss: 1.043551\n",
      "(Iteration 4521 / 5880) loss: 1.211009\n",
      "(Iteration 4531 / 5880) loss: 1.096799\n",
      "(Iteration 4541 / 5880) loss: 1.053930\n",
      "(Iteration 4551 / 5880) loss: 1.063290\n",
      "(Iteration 4561 / 5880) loss: 1.091006\n",
      "(Iteration 4571 / 5880) loss: 1.079121\n",
      "(Iteration 4581 / 5880) loss: 1.004718\n",
      "(Iteration 4591 / 5880) loss: 1.103106\n",
      "(Iteration 4601 / 5880) loss: 1.084867\n",
      "(Iteration 4611 / 5880) loss: 1.042019\n",
      "(Iteration 4621 / 5880) loss: 1.101660\n",
      "(Iteration 4631 / 5880) loss: 0.943005\n",
      "(Iteration 4641 / 5880) loss: 1.092590\n",
      "(Iteration 4651 / 5880) loss: 1.130440\n",
      "(Iteration 4661 / 5880) loss: 1.055496\n",
      "(Iteration 4671 / 5880) loss: 0.992498\n",
      "(Iteration 4681 / 5880) loss: 1.128863\n",
      "(Iteration 4691 / 5880) loss: 0.975856\n",
      "(Iteration 4701 / 5880) loss: 1.106222\n",
      "(Iteration 4711 / 5880) loss: 1.115472\n",
      "(Iteration 4721 / 5880) loss: 1.004220\n",
      "(Iteration 4731 / 5880) loss: 1.171349\n",
      "(Iteration 4741 / 5880) loss: 1.002095\n",
      "(Iteration 4751 / 5880) loss: 1.111058\n",
      "(Iteration 4761 / 5880) loss: 0.925906\n",
      "(Iteration 4771 / 5880) loss: 0.959231\n",
      "(Iteration 4781 / 5880) loss: 1.059446\n",
      "(Iteration 4791 / 5880) loss: 1.049890\n",
      "(Iteration 4801 / 5880) loss: 1.043513\n",
      "(Iteration 4811 / 5880) loss: 1.091844\n",
      "(Iteration 4821 / 5880) loss: 1.029992\n",
      "(Iteration 4831 / 5880) loss: 1.150245\n",
      "(Iteration 4841 / 5880) loss: 1.017020\n",
      "(Iteration 4851 / 5880) loss: 0.976122\n",
      "(Iteration 4861 / 5880) loss: 1.127701\n",
      "(Iteration 4871 / 5880) loss: 0.952415\n",
      "(Iteration 4881 / 5880) loss: 0.964154\n",
      "(Iteration 4891 / 5880) loss: 1.007072\n",
      "(Epoch 10 / 12) train acc: 0.782000; val_acc: 0.594000\n",
      "(Iteration 4901 / 5880) loss: 1.101329\n",
      "(Iteration 4911 / 5880) loss: 1.121582\n",
      "(Iteration 4921 / 5880) loss: 0.943801\n",
      "(Iteration 4931 / 5880) loss: 1.038867\n",
      "(Iteration 4941 / 5880) loss: 1.159643\n",
      "(Iteration 4951 / 5880) loss: 0.959810\n",
      "(Iteration 4961 / 5880) loss: 0.978434\n",
      "(Iteration 4971 / 5880) loss: 0.994356\n",
      "(Iteration 4981 / 5880) loss: 0.911528\n",
      "(Iteration 4991 / 5880) loss: 1.126560\n",
      "(Iteration 5001 / 5880) loss: 0.951332\n",
      "(Iteration 5011 / 5880) loss: 1.064172\n",
      "(Iteration 5021 / 5880) loss: 0.995018\n",
      "(Iteration 5031 / 5880) loss: 0.999978\n",
      "(Iteration 5041 / 5880) loss: 0.973854\n",
      "(Iteration 5051 / 5880) loss: 1.010971\n",
      "(Iteration 5061 / 5880) loss: 0.950404\n",
      "(Iteration 5071 / 5880) loss: 1.084412\n",
      "(Iteration 5081 / 5880) loss: 1.101107\n",
      "(Iteration 5091 / 5880) loss: 1.142095\n",
      "(Iteration 5101 / 5880) loss: 1.042907\n",
      "(Iteration 5111 / 5880) loss: 0.953723\n",
      "(Iteration 5121 / 5880) loss: 1.069880\n",
      "(Iteration 5131 / 5880) loss: 0.982701\n",
      "(Iteration 5141 / 5880) loss: 1.171445\n",
      "(Iteration 5151 / 5880) loss: 1.005335\n",
      "(Iteration 5161 / 5880) loss: 1.048979\n",
      "(Iteration 5171 / 5880) loss: 1.046910\n",
      "(Iteration 5181 / 5880) loss: 1.045466\n",
      "(Iteration 5191 / 5880) loss: 1.088892\n",
      "(Iteration 5201 / 5880) loss: 0.935953\n",
      "(Iteration 5211 / 5880) loss: 1.132475\n",
      "(Iteration 5221 / 5880) loss: 0.993758\n",
      "(Iteration 5231 / 5880) loss: 1.022355\n",
      "(Iteration 5241 / 5880) loss: 0.911872\n",
      "(Iteration 5251 / 5880) loss: 1.047082\n",
      "(Iteration 5261 / 5880) loss: 0.901542\n",
      "(Iteration 5271 / 5880) loss: 0.980904\n",
      "(Iteration 5281 / 5880) loss: 1.088579\n",
      "(Iteration 5291 / 5880) loss: 0.980357\n",
      "(Iteration 5301 / 5880) loss: 0.985157\n",
      "(Iteration 5311 / 5880) loss: 1.018177\n",
      "(Iteration 5321 / 5880) loss: 0.927878\n",
      "(Iteration 5331 / 5880) loss: 0.942133\n",
      "(Iteration 5341 / 5880) loss: 0.995819\n",
      "(Iteration 5351 / 5880) loss: 1.022666\n",
      "(Iteration 5361 / 5880) loss: 1.027964\n",
      "(Iteration 5371 / 5880) loss: 1.037886\n",
      "(Iteration 5381 / 5880) loss: 0.962423\n",
      "(Epoch 11 / 12) train acc: 0.771000; val_acc: 0.593000\n",
      "(Iteration 5391 / 5880) loss: 0.992244\n",
      "(Iteration 5401 / 5880) loss: 1.015440\n",
      "(Iteration 5411 / 5880) loss: 0.939923\n",
      "(Iteration 5421 / 5880) loss: 0.965291\n",
      "(Iteration 5431 / 5880) loss: 0.955022\n",
      "(Iteration 5441 / 5880) loss: 0.964269\n",
      "(Iteration 5451 / 5880) loss: 0.998370\n",
      "(Iteration 5461 / 5880) loss: 0.985742\n",
      "(Iteration 5471 / 5880) loss: 0.929568\n",
      "(Iteration 5481 / 5880) loss: 0.915399\n",
      "(Iteration 5491 / 5880) loss: 0.852728\n",
      "(Iteration 5501 / 5880) loss: 0.833048\n",
      "(Iteration 5511 / 5880) loss: 0.977504\n",
      "(Iteration 5521 / 5880) loss: 0.884408\n",
      "(Iteration 5531 / 5880) loss: 0.888056\n",
      "(Iteration 5541 / 5880) loss: 0.929074\n",
      "(Iteration 5551 / 5880) loss: 0.928215\n",
      "(Iteration 5561 / 5880) loss: 1.030044\n",
      "(Iteration 5571 / 5880) loss: 1.031070\n",
      "(Iteration 5581 / 5880) loss: 0.947765\n",
      "(Iteration 5591 / 5880) loss: 0.888169\n",
      "(Iteration 5601 / 5880) loss: 1.012654\n",
      "(Iteration 5611 / 5880) loss: 0.972008\n",
      "(Iteration 5621 / 5880) loss: 0.961638\n",
      "(Iteration 5631 / 5880) loss: 1.025412\n",
      "(Iteration 5641 / 5880) loss: 1.047114\n",
      "(Iteration 5651 / 5880) loss: 0.922109\n",
      "(Iteration 5661 / 5880) loss: 1.065510\n",
      "(Iteration 5671 / 5880) loss: 0.983366\n",
      "(Iteration 5681 / 5880) loss: 0.949970\n",
      "(Iteration 5691 / 5880) loss: 0.866857\n",
      "(Iteration 5701 / 5880) loss: 0.915742\n",
      "(Iteration 5711 / 5880) loss: 1.058945\n",
      "(Iteration 5721 / 5880) loss: 0.976681\n",
      "(Iteration 5731 / 5880) loss: 0.974281\n",
      "(Iteration 5741 / 5880) loss: 0.955246\n",
      "(Iteration 5751 / 5880) loss: 0.832415\n",
      "(Iteration 5761 / 5880) loss: 0.941268\n",
      "(Iteration 5771 / 5880) loss: 0.870171\n",
      "(Iteration 5781 / 5880) loss: 0.950489\n",
      "(Iteration 5791 / 5880) loss: 0.872254\n",
      "(Iteration 5801 / 5880) loss: 0.893479\n",
      "(Iteration 5811 / 5880) loss: 0.960418\n",
      "(Iteration 5821 / 5880) loss: 0.907090\n",
      "(Iteration 5831 / 5880) loss: 0.957874\n",
      "(Iteration 5841 / 5880) loss: 0.851634\n",
      "(Iteration 5851 / 5880) loss: 0.903313\n",
      "(Iteration 5861 / 5880) loss: 0.901423\n",
      "(Iteration 5871 / 5880) loss: 0.964784\n",
      "(Epoch 12 / 12) train acc: 0.799000; val_acc: 0.595500\n",
      "BEST VALID ACC: 0.596500\n"
     ]
    }
   ],
   "source": [
    "#the best implemntation for the NN\n",
    "layers=[1200, 750, 250]\n",
    "best_model = FullyConnectedNet(layers ,use_batchnorm=True,reg=0.001, weight_scale=2.511886e-02,\n",
    "                                  dtype=np.float64)\n",
    "\n",
    "Trained = Trainer(best_model, data,\n",
    "                num_epochs=12, batch_size=200,\n",
    "                update_rule='adam',\n",
    "                optim_config={\n",
    "                'learning_rate':  1.533608e-4\n",
    "                },\n",
    "                verbose=True)\n",
    "Trained.train()\n",
    "print(\"BEST VALID ACC: %f\" % Trained.best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/air/anaconda3/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py:106: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  warnings.warn(message, mplDeprecation, stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA20AAANsCAYAAADIrkBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1515db30b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3IAAAETCAYAAABz4DQ3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBHckAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15170517f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3IAAAETCAYAAABz4DQ3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBHckAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1517d9a400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(Trained.loss_history, 'o', label=\"Best model\")\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(Trained.train_acc_history, '-o',label=\"Best model\")\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(Trained.val_acc_history, '-o',label=\"Best model\")\n",
    "\n",
    "for i in [1, 2, 3]:\n",
    "    plt.subplot(3, 1, i)\n",
    "    plt.legend(loc='upper center', ncol=4)\n",
    "    plt.gcf().set_size_inches(15, 15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set accuracy:  0.6125\n",
      "Test set accuracy ACCR:  0.5879\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = np.argmax(best_model.loss(data['X_test']), axis=1)\n",
    "y_val_pred = np.argmax(best_model.loss(data['X_val']), axis=1)\n",
    "print('Validation set accuracy: ', (y_val_pred == data['y_val']).mean())\n",
    "print('Test set accuracy ACCR: ', (y_test_pred == data['y_test']).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCRn of plane is:  0.629000\n",
      "CCRn of car is:  0.692000\n",
      "CCRn of bird is:  0.439000\n",
      "CCRn of cat is:  0.416000\n",
      "CCRn of deer is:  0.555000\n",
      "CCRn of dog is:  0.475000\n",
      "CCRn of frog is:  0.652000\n",
      "CCRn of horse is:  0.636000\n",
      "CCRn of ship is:  0.726000\n",
      "CCRn of truck is:  0.659000\n"
     ]
    }
   ],
   "source": [
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "CCRn = np.zeros(10)\n",
    "for i in range(10):\n",
    "        numCorrect = np.sum(y_test_pred[x] == Y_test[x] and y_test_pred[x] == i for x in range(X_test.shape[0]))\n",
    "        CCRn[i] = float(numCorrect) / 1000\n",
    "        print ('CCRn of %s is:  %f' % (classes[i], CCRn[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
